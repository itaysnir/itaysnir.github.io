---
layout: post
title:  "Pwn College - Microarchitecture Exploitation"
date:   2024-05-25 19:59:45 +0300
categories: jekyll update
---

**Contents**
* TOC
{:toc}
## Overview

This module deals with microarchitectural vulns, including `Meltdown` and `Spectre`. All challenges involves using these vulns to create kernel exploits such that either information disclosure, or even a full PE would be made. \
This module is insanely `1337`, as it requires deep hardware knowledge, in addition to good foundations with kernelspace and userspace exploitation. 

## Background

The focus of this module is bugs that occur **below assembly**, meaning - within the hardware itself. \
I've heavily relied on [this paper][hwforhackers] and [this stanford presentation][stanford-pres], and [this][c++-consistency]. \
In modern CPUs, memory instructions may not touch the RAM at all:

```bash
mov rax, [rax + 0x20]
```

As a massive optimization, caching of memory is being used, mapping between memory and its content - in order to avoid costly accesses to the RAM itself. \
Typically, there are multiple layers of caches, some of them are shared among cores (`L3 cache`) and some are core-specific (`L1 cache`). \
The instruction `mfence` is related to data consistency models. There are few models: `relaxed, consume, acquire, release, acquire-release, sequential("classic")`, and some extra possible instructions - `lfence, sfence`. \
`mfence` ensures that every load and store instruction that precedes the `mfence` instruction is globally visible before any load or store instruction that follows the `mfence` instruction. 

### Cache <-> RAM Consistency (WC, WT, WB)

Whenever a processor wants to write a word, it checks to see if the address it wants to write the data to is present in the cache or not. If it presents, **write hit**. We can update the value in cache, but it results in inconsistent data problem, as both cache and main memory have different data. This is called **Write Cache (WC)**. \
Another option is **Write Through (WT)** - where the data is updated simultaneously to cache and memory, simple and reliable, usually used when there aren't many writes relative to reads. \
Another option is **Write Back (WB)** - the data is updated only in the cache, and updated into the memory at a **later time**. Data is updated in the memory only when the cache line is going to be replaced. Therefore, a **Dirty Bit** is being introduced for every block (cacheline) in the cache - indicating if the data present in the cache was modified (dirty) or clean. If it is clean, there's no need to write it into the memory. Every write to the cache would enable the dirty bit, indicating it needs to be updated within the RAM. 

### Write Miss

Upon write miss, where we'd like to write into a location that isn't within the cache, we may use one of 2 approaches:

1. **Write Allocation** - data is loaded from the memory into cache, and then updated. works with WB and WT, but generally used with WB - as it is unnecessary to bring cacheline from memory to cache, and then update both in cache and RAM. WT is often used with no write allocate.

2. **Write Around** - data is directly written to RAM without interacting with the cache at all. When data isn't immediately used again, this is a decent choice. 

### Cache Coherence

Recall modern computer systems has a separate cache for every CPU under a multiprocessed environment. Coherence deals with the problem of **making different CPU's caches (possibly multilevel), as well as the RAM, to be seen as a one big flat memory location, for all processors**. In other words, all processors must agree on the order of R/W operations on a given adddress `X`. 

The fixed-length blocks in which cache and memory transfers data is a cache-line, usually 128 or 256 bytes long. \
Moreover, each cache is made of ways and sets. We can think of **sets** as hash-table buckets, and **ways** as maximal number of elements per bucket. \
Cache coherency protocols manage cacheline states to prevent inconsistent or lost data, the popular among them is MESI. Its idea is having four states, `modified, exclusive, shared, invalid`, denoted by two bit `tag` within the cache (in addition to its physical address and data). 

- Modified - the corresponding CPU of that cache have made some write operation on that cacheline. This means this memory is guaranteed not to appear in any other CPU cache. Hence, owned by that CPU. This state is only entered upon perfoming a write to the cacheline. Upon receving "read invalidate" message, the cacheline would be snooped to other CPU's cache. 

- Exclusive - very similar to modified, but the cache line has not yet been modified by the corresponding CPU (meaning, the copy of the cacheline data within the RAM is up-to-date). Before achieving this state, a writing CPU shall send "read invalidate" message within the interconnecting bus, hence invalidating any cachelines in "shared" state. 

- Shared - a cacheline might be replicated in at least one other CPU cache. Meaning, the CPU is not permitted to store to the line without first consulting with other CPUs. A fresh new load of a cacheline, starts within this state. 

- Invalid - empty line, holds no data. Usually new fetched cacheline would replace invalid state cachelines. 

While the internals of the MESI protocol aren't trivial, the key note is that **CPU's cache coherence is a solved problem**. This means that correctness is preserved, and ALWAYS, all CPUs, (and of course the operating system) sees the same view of memory, having the illusion of a flat memory model. 

### Write Stalls

Recall the MESI protocol actually requires that upon CPU-0 write, it shall send an "invalidate message" for other CPUs, wait for their operation, and wait for their "ACK message". Only then, its execution shall continue. \
There are multiple ways to prevent unnecessary stalling of writes:

1. Store Buffers - add store uffers between each CPU and its cache, recording its write in a store buffer and continue execution. 

2. Store Forwarding - the idea is for each CPU to snoop its store buffer and its cache, when performing loads. Meaning, a given CPU's stores are directly forwarded to its subsequent loads, without passing through the cache. 

3. Memory Barriers - handles violation of global memory ordering.

Example:

```c
int a = 0;
int b = 0;

void foo()
{
    a = 1;
    b = 1;   
}

void bar()
{
    while (b == 0) continue;
    assert(a == 1);
}
```

CPU0 executes `foo`, CPU1 executes `bar`. The cacheline containing `a` resides only in CPU1-cache, and the cacheline containb `b` differs, and owned by CPU0. \
The root problem with this case, is that because `a`'s cacheline not in CPU0 cache, it would place the new value of `a` within its store-buffer, and transmit "read invalidae" message. Hence, when CPU1 checks the value of `a`, incase this operation occurs before the "read invalidate" message, it finds its un-updated value of `0` within its cache, and the assertion fails. \
This is the memory consistency problem, and as opposed to coherence, the HW designers have nothing direct to help here, as the CPUs have no idea which variables are related. **Memory consistency deals with apparent ordering for all locations - meaning, order in which memory operations performed by one processor become visible to other threads**. As opposed to coherence, it defines the behavior of R/W operations to **different** memory locations, as observed by other processors. \
However, memory-barrier instructions allows the software to tell the CPU about such relations. In our case, adding `smp_mb()` right after the write to `a` would cause the CPU(0) to **flush its store buffer**, after `a = 1`. It could either do so by stalling until the store buffer is empty, or marking all entries within store buffer until `smp_mb` was called (stores after this call would be unmarked) - even if the relevant stores have their cacheline in the corresponding CPU's cache. 

### R/W Memory Barriers

`smp_mp` actually marks both store buffers and invalidate queues. A possible optimization is having weaker memory-barrier instructions - `smp_wmb` for writes and `smp_rmb` for reads. \
The effect of `smp_wmb`, is that the barrier only order writes - all stores preceding that memory barrier will appear to have completed before any stores after that barrier - but **only on the CPU that executes it**. The `smp_rmb` is similar - preceding reads are guranteed to be completed before reads that occur past the barrior, but only on the CPU that have executed the barrier instruction. 

### x86, AMD64 Memory Consistency Model

The consistency model of x86 and AMD64 are considered very strong, almost SC (sequential consistency), and called TSO (total store ordering). \
While most reorderings aren't enabled, one type of reodering is enabled, which is **stores may be reordered to be after loads**. \
x86 enables that reordering too, but also enables incoherent instruction cache / pipeline, unlike AMD64. \
TSO states that the store instructions may be buffered in a store buffer, and remain there until they will be stored to the memory. The read operation which follows the write in the program order may execute first, and search to the store buffer of the last write (to handle the case of write then read of the same address). All other memory operations follows the program order. \
In order to preserve correctness, read-modify-write instructions must be implemented (by replacing the write / read instruction). By doing so, the OOO execution is avoided. \
As an example, imagine the following:

```c
// P1
A = 1
x = B

// P2
B = 1
u = A
```

Under SC, the outcome `(x, u) == (0, 0)` isn't possible, as non of the processors may perform reordering, hence either `A` or `B` is always set. However, for TSO, the output of `(0, 0)` is possible. Because both processors performs store and then load (to different memory addresses), reordering may occur (even in the processor's view). This means that, by `P1`'s perspective, **it may think `P2` have reordered its operations** as follows:

```c
u = A
B = 1
```

In such case, by the whole global system's perspective, the following ordering may occur:

```c
u = A  // P2
A = 1  // P1
x = B  // P1
B = 1  // P2
```

Hence, `(0, 0)` is a possible outcome. 

Regarding locking, each processor that is willing to access a critical code section has to check if any other processor has locked it, in order to preserve exclusive access to the memory. The AMD64 implementation of Linux's `smp_mb` primitive is `mfence` (neither stores nor loads may be reordered), `smp_rmb` is `lfence` (loads cannot be reordered), and `smp_wmb` is `sfence` (stores cannot be reordered). \
Notice that `smp_mb, smp_rmb, smp_wmb` also force the compiler to eschew any optimizations that would have the effect of reordering. \

### ARM Memory Consistency Model

As opposed to x86, ARM-v7 processors has **very relaxed consistency model**, similar to POWER processors. This actually means that **ALL possible reoderings are possible**, with the only exception of dependent loads reordering are disabled (for example, reading from a linked list `next` pointer). \
The following [paper][arm-consistency] and [this short paper][arm-short] may come handy to gain deeper knowledge about this. \
Notice that different ARM versions may have other, stronger consistency models (such as ARM-v8). 

### Locks, `std::atomic`

In addition to fences, other (much more efficient) solutions are using locks, as well as C++'s `<atomic>` library. The idea is that writing fences by hand is usually very bad paradigm, especially as the compiler can do it for us - using mutexes and `std::atomic<>` variables. \
I won't get into the details, as it's out of the scope of this module, however some of the referenced slides contains great explanations about these. 

### Volatile

The following fact holds:

```c
volatile(Java, .NET) == atomic(C, C++) != volatile(C, C++) 
```

While mutexes, `std::atomics`, and memory barriers are synchronizing operations for other tasks within the same program (same `mm`), `volatile` is actually compiler-only barriers, **unoptimizable variables** made for "talking to memory locations outside of the program" (different `mm`), such as HW registers, GPU code, etc. \
The motivation behind `volatile` is to allow access outside of the current thread of execution, such as to MMIO devices, preserving values across `longjmp`, and sharing values between signal handlers and the rest of the program. In particular, this means the variable declared as `volatile` is stored in memory, not registers. 

### Example

Assume the following code:

```c
#include <x86intrin.h>

#define BUF_SIZE 8

char buf[BUF_SIZE];

int foo()
{
    uint64_t start, end;
    _mm_clflush(buf);
    _mm_mfence();

    start = __rdtsc();

    // First access
    volatile uint64_t x = buf[((buf[0] + 4) ^ 1) ^ 1];
    _mm_mfence();

    end = __rdtsc();
}
```

`clflush` flushes the VA of buf from memory cache to RAM. `mfence` is a memory barrier for R/W, and `rdtsc` simply queries the clock time. \
Note the wierd access pattern for `buf`, writing to `x` (yet NOT reading its value at all!). Its reason it to prevent prefetching. Moreover, `x` is volatile - meaning there are no compiler optimizations for it, and saves it within the memory and not a register. \
If we would repeat the exact same access to `buf` (without `clflush`), storing it within `x`, we would obtain faster access times for the preceding calls. 


### Speculative Execution

Imagine the following code:

```bash
mov rdi, 0
mov rax, [rax + 0x20]
cmp rax, 0
jz label
    mov rdi, 1
label:
    mov [rsi + 0x30], rdi
```

Accessing the memory `[rax + 0x20]` is very slow. While waiting for this operation to complete, the CPU can guess the branch to be taken. If incorrect, redo computation. This means that the CPU may assume `rax == 0`, and speculate-execute the `mov [rsi + 0x30]` operation. If we're incorrect, redo would happen. The relevant HW unit that chooses whether or not branching should be made, is the "branch prediction unit", which a sophisticated attacker may influence. \
The CPU caching / speculative execution behavior varies upon model. Can be seen at `/proc/cpuinfo` and `lscpu`. The size of a cacheline is denoted as `clflush size`. \
The problem is this, is the side channel of caching. In particular, based on the timing, we can infer whether or not caching have occured. Since the caching layer is in the CPU, NOT the process - If we have multiple programs that accesses the same shared memory (the same underlying page) - we can determine whether or not that value has accessed across processes. \
A very cool linux command is `lstopo`, which shows the topology of the system. This includes the various of cores, as well as their hyperthreading count (more than one hardware thread on a physical core), L1-I (instruction) and L1-D (data) caches, as well as L2 and L3 caches (and disk). The L3 cache is usually shared among cores. \
Notice that in case we have some side channel effect with some cache we'd like to observe, it is important whether or not this cache is CPU-specific, or shared among CPUs. If it is specific to a CPU, we have to make sure the running programs / threads are all having the same affinity - meaning they're all using the same cache. 

### Flush and Reload







[hwforhackers]: https://www.puppetmastertrading.com/images/hwViewForSwHackers.pdf
[stanford-pres]: https://gfxcourses.stanford.edu/cs149/winter19content/lectures/09_consistency/09_consistency_slides.pdf
[arm-consistency]: https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf
[arm-short]: https://research.swtch.com/hwmm
[c++-consistency]: https://onedrive.live.com/view.aspx?resid=4E86B0CF20EF15AD!24884&app=WordPdf&authkey=!AMtj_EflYn2507c
